{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNpcgYlJ8oGTYpbDFT/5k6o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4DhBicUVjf0u"},"outputs":[],"source":["from google.colab import drive  \n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Kuliah Teknik Informatika/Semester 5C/CAPSTONE PROJEK/CHATBOT'"],"metadata":{"id":"kNwantcGj_7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"yr9cX5yBm9FU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras"],"metadata":{"id":"jp_98jqEvaxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"EcAv4OrIyHX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from tensorflow.keras.optimizers import SGD\n","from keras.layers import Dense, Dropout\n","from keras.models import load_model\n","from keras.models import Sequential\n","import numpy as np\n","import pickle\n","import json\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"82ksFpD-kT6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('omw-1.4')\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")"],"metadata":{"id":"WgNN6kdskWBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Membuat Variabel\n","words = []\n","classes = []\n","documents = []\n","ignore_words = [\"?\", \"!\"]\n","data_file = open(\"/content/drive/MyDrive/Kuliah Teknik Informatika/Semester 5C/CAPSTONE PROJEK/CHATBOT/intents.json\").read()\n","intents = json.loads(data_file)"],"metadata":{"id":"UlP7mnzykYQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for intent in intents[\"intents\"]:\n","    for pattern in intent[\"patterns\"]:\n","\n","        # mentokenisasi setiap kata\n","        w = nltk.word_tokenize(pattern)\n","        words.extend(w)\n","        # menambahkan documents kedalam corpus\n","        documents.append((w, intent[\"tag\"]))\n","\n","        # menambahkan ke list classes kita\n","        if intent[\"tag\"] not in classes:\n","            classes.append(intent[\"tag\"])"],"metadata":{"id":"JJEi3FJCkaPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lemmatize pada setiap kata\n","words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","classes = sorted(list(set(classes)))\n","\n","print(len(documents), \"documents\")\n","\n","print(len(classes), \"classes\", classes)\n","\n","print(len(words), \"unique lemmatized words\", words)\n","\n","#menyimpan olahan kata kedalam bentuk model\n","pickle.dump(words, open(\"words.pkl\", \"wb\"))\n","pickle.dump(classes, open(\"classes.pkl\", \"wb\"))"],"metadata":{"id":"ygHPrDR6kcSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# membuat training data dan array kosong untuk output\n","training = []\n","output_empty = [0] * len(classes)\n","for doc in documents:\n","    # initializing bag of words\n","    bag = []\n","    # list of tokenisasi untuk setiap pola\n","    pattern_words = doc[0]\n","    # lemmatize setiap kata - create kata dasar, agar menjadi key bagi bentuk kata ubahnya\n","    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n","    # create our bag of words array with 1, jika keccocokan kata di temukan dalam pola saat ini\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","\n","    # output is a '0' for setiap tag and '1' for tag saat ini(untuk setiap pola)\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])\n","# acak semua features and ubah kedalam bentuk np.array\n","random.shuffle(training)\n","training = np.array(training)\n","# create train and test lists. X - patterns, Y - intents\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])\n","print(\"Training data created\")"],"metadata":{"id":"dF6fKPq0kf6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# implemntasi sequintial pada data train\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation=\"softmax\"))\n","model.summary()"],"metadata":{"id":"Q6UxBx-IkiSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# algoritma untuk menentukan nilai akurasi\n","sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])"],"metadata":{"id":"NHdfpS4SkjX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"," from keras import callbacks\n"," earlystopping = callbacks.EarlyStopping(monitor =\"loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n","callbacks =[earlystopping]"],"metadata":{"id":"J-qijCmlkoBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # fitting dan simpan model\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","model.save(\"chatbot_model.h5\", hist)\n","print(\"model created\")"],"metadata":{"id":"AB4ywEQ_kqMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# membersikan kata dari simbol yang tidak diperlukan\n","def clean_up_sentence(sentence):\n","  # tokenisasi pola-memisahkan kata kedalam bentuk array\n","    sentence_words = nltk.word_tokenize(sentence)\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","\n","# membuat function kamus bag of words. array: 0 or 1 for each word in the bag that exists in the sentence\n","def bow(sentence, words, show_details=True):\n","    # tokenize pola\n","    sentence_words = clean_up_sentence(sentence)\n","    # bag of words - matrix of N words, vocabulary matrix\n","    bag = [0] * len(words)\n","    for s in sentence_words:\n","        for i, w in enumerate(words):\n","            if w == s:\n","                # isi 1 if jika kata saat ini ada di posisi kosakata\n","                bag[i] = 1\n","                if show_details:\n","                    print(\"found in bag: %s\" % w)\n","    return np.array(bag)"],"metadata":{"id":"Y1aA_PziksJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# membuat function prediksi jawaban\n","def predict_class(sentence, model):\n","    # menyaring prediksi dibawah threshold\n","    p = bow(sentence, words, show_details=False)\n","    res = model.predict(np.array([p]))[0]\n","    ERROR_THRESHOLD = 0.25\n","    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n","    # mengurutkan berdasarkan probabilitas\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n","    return return_list\n","\n","# membuat function untuk memberi respon\n","def getResponse(ints, intents_json):\n","    tag = ints[0][\"intent\"]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","        if i[\"tag\"] == tag:\n","            result = random.choice(i[\"responses\"])\n","            break\n","    return result"],"metadata":{"id":"hJGwe_tHkve9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# menampilkan respon berdasarkan prediksi\n","def chatbot_response(msg):\n","    ints = predict_class(msg, model)\n","    res = getResponse(ints, intents)\n","    response = print(res)\n","    return response"],"metadata":{"id":"kBg-PqLQkxy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    input_data = input(\"You Message : \")\n","    if input_data == ('exit'):\n","        print(\"Terima kasih\")\n","        break\n","    answer = chatbot_response(input_data)\n","    print(\"Bot answer : \", answer)"],"metadata":{"id":"lHkGf8b3ky_i"},"execution_count":null,"outputs":[]}]}